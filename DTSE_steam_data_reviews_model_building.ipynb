{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DTSE-steam-data-reviews-model-building.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nSVdsz9NZe8SAj-5h-jnt7vpt_XWi7rG",
      "authorship_tag": "ABX9TyOnOPkjRSh95L1i9Wu7rsfY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stazam/DTSE-project/blob/main/DTSE_steam_data_reviews_model_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model bulding**\n",
        "\n",
        "First we will start by preprocessing text variables in datasets:\n",
        "\n",
        "1. checking english language - we will be modelling only english written variables\n",
        "2. removing some special characters\n",
        "3. \n",
        "\n",
        "We will try to use various models for user_review category prediction. Types of models which we will consider:\n",
        "\n",
        "1. Bidirectional LSTM layer (+ CNN layer) with embedding layer without removing stop words and lematization \n",
        "2. Bidirectional LSTM layer (+ CNN layer) with embedding layer with using a stop wrods and lematization \n",
        "3. BERT - transformer for text classification without pretrained embeddings without removing stop words and lematization. \n",
        "4. BERT - transformer for text classification with pretrained embeddings without removing stop words and lematization.\n",
        "\n",
        "Also we will use stacking method to incorporate information from varibales **year**. \n",
        "\n",
        "At the end we create **predict_pipeline** function for prediction, which will consist of preprocessing and prediction part.\n",
        "\n"
      ],
      "metadata": {
        "id": "3i_Q7TyH9h2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ow-E-pVtvsP",
        "outputId": "97b453a4-866c-415e-b54a-c5d7f9731464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=a07ed3aec9f79e4769477dc58788fdcab8b6b6161f13c295d1bef1824ddc97fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import sys\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from keras import applications\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional\n",
        "from keras.layers import Convolution2D, MaxPooling2D,BatchNormalization,GlobalAveragePooling1D, Flatten, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/MyDrive/DTSE-data/Python-files')\n",
        "from help_functions import *"
      ],
      "metadata": {
        "id": "pIEI8zMvvO19"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_merged = pd.read_pickle(\"/content/drive/MyDrive/DTSE-data/data-files/df_train_merged.pkl\")\n",
        "df_test_merged = pd.read_pickle(\"/content/drive/MyDrive/DTSE-data/data-files/df_test_merged.pkl\")"
      ],
      "metadata": {
        "id": "HqwzYg_9vO4N"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Bidirectional LSTM** (+ CNN) without removing stop words and lematization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ebEr8YQ-AIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text** preprocessing\n",
        "\n",
        "We will check for language of string variables. Since we have seen during **game_overview** file inspection that both *developer, publisher* and also *title* are english words, we do not need to check them. But we have to inspect **user_review** for english language."
      ],
      "metadata": {
        "id": "7-jlcNLkm2Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_language(x):\n",
        "    \n",
        "    try:\n",
        "      return detect(x) \n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_train_merged['languge'] = df_train_merged.user_review.apply(lambda x: check_language(x))"
      ],
      "metadata": {
        "id": "bpN9bQdJ9Gc7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('{} rows from train dataset in different language were removed'.format(sum(df_train_merged.languge.values != 'en')))\n",
        "df_train_merged = df_train_merged.loc[df_train_merged.languge == 'en',:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LSHr7Z0x0xF",
        "outputId": "b11e3953-d516-40d4-a78a-d7df7147dcbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215 rows from train dataset in different language were removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we loose 215 rows with user_reviews in differnt languages."
      ],
      "metadata": {
        "id": "gi11pFHZ6tCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re "
      ],
      "metadata": {
        "id": "-ZMwG-Kx8Yzz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#symbols = set([y for x in df_train_merged.user_review.values for y in x if not y.isalnum()])\n",
        "#print(symbols)\n",
        "symbols_up = {'⭐', '`', 'ี', '／', '“', '⟶', '‘', '˘', '\\u2007', '□', '♬', '\\xad', '«', '้', '☢', '[', '¯', '＼', ']', '☠', 'Ⓚ', 'ั', '¶', '€', '⌐', '^', '‹', '⁀', '･', '£', '%', '=', ' ', '*', '¬', '▀', '━', '♪', '⌬', '∞', '&', ';', '”', '・', '▒', '+', '☐', '{', '¨', '⌠', '☹', '\\xa0', '｜', '~', '？', '」', '¿', '¢', '่', '|', '◕', '@', '‡', '⟵', '☻', '·', '≠', '´', '☑', '✎', '۞', '͠', '˚', '✖', '●', '✦', '✰', '⋎', '͜', '✫', '■', '✩', '،', '┐', '►', \"'\", '_', '→', '❤', '§', '︵', '░', '✌', '▽', '•', '！', '▲', '█', '?', '▐', '˜', '͟', '}', '♥', '▄', '☣', '＿', '〃', '\\u3000', '＾', '☼', '★', '®', '┌', '¤', '☺', '\\\\', '▌', 'ู', '「', '¡', '’', '▼', '¦', '°', '┳', '…', '#', '±', '～', '\\u200b', '>', '⚠', '⌒', '‰', '<', '©', '✓', '‵', '™',  'ื', '\\ufeff', 'ⓘ', '。', '‿', '︻', '͡', 'Ⓞ', '↑', '/', '¥', '$', '⊂', '═', '♫', '☆', 'ิ', '†', '̶', '↓', '✔', '┻'}"
      ],
      "metadata": {
        "id": "ic8hpuoXqWkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(re.findall('[A-Z][^A-Z]*', 'TakToto je super SkuskaOdvahy'))"
      ],
      "metadata": {
        "id": "4xlgIyCXp3GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.replace(\"'s\",\" is\").replace(\"'re\",\" are\").replace(\"'ll\",' will')"
      ],
      "metadata": {
        "id": "FJX_2Dvvp3IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,100):\n",
        "  for x in symbols_up:\n",
        "    df_train_merged.user_review[i] = df_train_merged.user_review[i].replace(x,' ')\n",
        "  print(df_train_merged.user_review[i]) "
      ],
      "metadata": {
        "id": "bgc8zAXnqTVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.sub(r\"\\s{2,}\", \" \",a)"
      ],
      "metadata": {
        "id": "1d4zobpXp3Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tuZLcSmBp3OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_merged['text'] = df_train_merged.title + df_train_merged.developer + df_train_merged.publisher + df_train_merged.tags.apply(lambda x: x[1:-1]) + df_train_merged.overview + df_train_merged.user_review\n",
        "df_test_merged['text'] = df_test_merged.title + df_test_merged.developer + df_test_merged.publisher + df_test_merged.tags.apply(lambda x: x[1:-1]) + df_test_merged.overview + df_test_merged.user_review\n",
        "\n",
        "X = df_train_merged\n",
        "y = X.user_suggestion.values"
      ],
      "metadata": {
        "id": "tS9ZP9hFIu0G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will start by adding a variable **text** which will be composed of variables *title, developer, publisher, tags, overview, user_review.*\n",
        "\n",
        "I also consider training two separate models, first just on *user_reveiew* and second on the rest of mentioned variables and then stack results rogether, but I belive it wouldn't improve performance of model, because tags, developer, publisher, title are the same rows in the data set, once when the targer variable **user_suggestion** is 1 and once when 0, so the model would be confused. "
      ],
      "metadata": {
        "id": "e4P1tUGJvHjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "Pnxt3PiGl1d-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_input = []\n",
        "for x in X_NN:\n",
        "  X_NN = X_NN.replace('\\'','')\n",
        "  X_NN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ccioUAQIl1hR",
        "outputId": "7b22632d-4d7d-4c19-87eb-da9adac3dfaa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4a56e15fb3d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'replace'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_NN = X_train.text.values\n",
        "X_test_NN = X_test.text.values"
      ],
      "metadata": {
        "id": "gfDTzbJS5fqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The longest sequence is {} words long.\".format(max([len(x) for x in X_train_NN])))\n",
        "pd.DataFrame([len(x) for x in X_train_NN]).describe()"
      ],
      "metadata": {
        "id": "Hhhh9h3k54aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 5000\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_size = 10000\n",
        "#training_size = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train_NN)\n",
        "\n",
        "print('Vocabulary is {} words large'.format(len(tokenizer.word_index)))"
      ],
      "metadata": {
        "id": "5O2kMOj657l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(tokenizer.word_counts.items(), key = lambda t: t[1])[47000:]"
      ],
      "metadata": {
        "id": "AMC5vyJm57rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_NN = tokenizer.texts_to_sequences(X_train_NN)\n",
        "X_train_padded_NN = np.array(pad_sequences(X_train_NN, maxlen=max_length, padding=padding_type, truncating=trunc_type))\n",
        "\n",
        "X_test_NN = tokenizer.texts_to_sequences(X_test_NN)\n",
        "X_test_padded_NN = np.array(pad_sequences(X_test_NN, maxlen=max_length, padding=padding_type, truncating=trunc_type))\n"
      ],
      "metadata": {
        "id": "CPGjm-qV57sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size,240, input_length = max_length))\n",
        "model.add(Bidirectional(keras.layers.LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(keras.layers.LSTM(32, return_sequences=True)))\n",
        "model.add(Flatten()),\n",
        "Dropout(0.5),\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics = METRICS)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9jV_3ngl57yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "history = model.fit(X_train_padded_NN, y_train, epochs=1, validation_data=(X_test_padded_NN, y_test), verbose = 1)"
      ],
      "metadata": {
        "id": "aTaOdjMs57y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pred = model.predict(X_train_padded_NN)\n",
        "X_test_pred = model.predict(X_test_padded_NN)"
      ],
      "metadata": {
        "id": "EIRLI2am57zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "def knn_inputer(X):\n",
        "\n",
        "    X[X.isnull()] = np.nan\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    X_im = imputer.fit_transform(X)\n",
        "    return pd.DataFrame(X_im, columns = list(X.columns))\n",
        "\n",
        "def process_dat(X):\n",
        "  \n",
        "  X_1 = X.loc[:,['title','developer','publisher']]\n",
        "  X_1 = pd.get_dummies(X_1)\n",
        "  X_1 = pd.concat([X_1,X.loc[:,['review_id','year']]], axis = 1)\n",
        "\n",
        "  return X_1\n",
        "\n",
        "X_trainm = process_dat(X_train)\n",
        "X_trainm = knn_inputer(X_trainm)\n",
        "X_trainm['predictions'] = [x[0] for x in X_train_pred.tolist()]\n",
        "\n",
        "X_testm = process_dat(X_test)\n",
        "X_testm = knn_inputer(X_testm)\n",
        "X_testm['predictions'] = [x[0] for x in X_test_pred.tolist()]\n",
        "\n",
        "print(X_trainm.isnull().sum())\n",
        "print(X_testm.isnull().sum())"
      ],
      "metadata": {
        "id": "ZAqa4zPW573O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Bidirectional LSTM** (+ CNN) with removing stop words and lematization\n"
      ],
      "metadata": {
        "id": "Op91xPM7c45j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "riaCc_S85763"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BDI8iBT96Pyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TBGvudz46P1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.text.values[100].replace('\\'','')"
      ],
      "metadata": {
        "id": "phcmkZRT6P4z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "69b0eaa0-e70b-4765-978f-d15c5a3fee6b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dota 2Valve Valve \\'Free to Play\\', \\'MOBA\\', \\'Strategy\\', \\'Multiplayer\\', \\'Team-Based\\', \\'Action\\', \\'e-sports\\', \\'Online Co-Op\\', \\'Competitive\\', \\'PvP\\', \\'RTS\\', \\'Difficult\\', \\'RPG\\', \\'Fantasy\\', \\'Tower Defense\\', \\'Co-op\\', \\'Character Customization\\', \\'Replay Value\\', \\'Action RPG\\', \\'Simulation\\'The most-played game on Steam.Every day, millions of players worldwide enter battle as one of over a hundred Dota heroes. And no matter if it\\'s their 10th hour of play or 1,000th, there\\'s always something new to discover. With regular updates that ensure a constant evolution of gameplay, features, and heroes, Dota 2 has truly taken on a life of its own.One Battlefield. Infinite Possibilities.When it comes to diversity of heroes, abilities, and powerful items, Dota boasts an endless array—no two games are the same. Any hero can fill multiple roles, and there\\'s an abundance of items to help meet the needs of each game. Dota doesn\\'t provide limitations on how to play, it empowers you to express your own style.All heroes are free.Competitive balance is Dota\\'s crown jewel, and to ensure everyone is playing on an even field, the core content of the game—like the vast pool of heroes—is available to all players. Fans can collect cosmetics for heroes and fun add-ons for the world they inhabit, but everything you need to play is already included before you join your first match.Bring your friends and party up.Dota is deep, and constantly evolving, but it\\'s never too late to join. Learn the ropes playing co-op vs. bots. Sharpen your skills in the hero demo mode. Jump into the behavior- and skill-based matchmaking system that ensures you\\'ll be matched with the right players each game.>do presentation in school>topic is \"what inspires you most\">pick memes>do a passionate speech about how memes inspire my dota play>my voice is shaking>nobody says a word when i finish>feel extremely awkward>nobody really talks to me after class, some girls giggle>when i leave teacher holds me back and just quickly whispers \"i love memes too\">we just smile at each other for some seconds before i leavei feel so alive right now'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test(x):\n",
        "  \n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  x = x.replace(\"'re\",'are')\n",
        "  x = x.replace(\"'s\",'is')\n",
        "  x = x.replace(\"'n\",'not')\n",
        "  x = x.replace(\"'n\",'not')\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(x)\n",
        "\n",
        "  filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w.lower() in stop_words]\n",
        "  \n",
        "  return \" \".join(filtered_sentence)"
      ],
      "metadata": {
        "id": "_qmxuOMRJDpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in skuska:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd4FSTDlLQ_x",
        "outputId": "56b62673-d247-48b8-bec8-bc0a68e29c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "41VoGRnUIu5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AYBSTf_fIu7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_merged['text'] = df_train_merged.tags.apply(lambda x: x[1:-1]) + df_train_merged.overview + df_train_merged.user_review\n",
        "\n",
        "X = df_train_merged\n",
        "y = X.user_suggestion.values\n",
        "X.head()"
      ],
      "metadata": {
        "id": "u5iHwcYe9bVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "oDPYKWab9bX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_NN = X_train.text.values\n",
        "X_test_NN = X_test.text.values"
      ],
      "metadata": {
        "id": "SERRxA419bbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The longest sequence is {} words long.\".format(max([len(x) for x in X_train_NN])))\n",
        "pd.DataFrame([len(x) for x in X_train_NN]).describe()"
      ],
      "metadata": {
        "id": "RqjELYpU-NWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 5000\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_size = 10000\n",
        "#training_size = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train_NN)\n",
        "\n",
        "print('Vocabulary is {} words large'.format(len(tokenizer.word_index)))"
      ],
      "metadata": {
        "id": "ItFIFbEV-NYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(tokenizer.word_counts.items(), key = lambda t: t[1])[47000:]"
      ],
      "metadata": {
        "id": "MijxKvnX-Naf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_NN = tokenizer.texts_to_sequences(X_train_NN)\n",
        "X_train_padded_NN = np.array(pad_sequences(X_train_NN, maxlen=max_length, padding=padding_type, truncating=trunc_type))\n",
        "\n",
        "X_test_NN = tokenizer.texts_to_sequences(X_test_NN)\n",
        "X_test_padded_NN = np.array(pad_sequences(X_test_NN, maxlen=max_length, padding=padding_type, truncating=trunc_type))"
      ],
      "metadata": {
        "id": "3tejFnzP-Nc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size,240, input_length = max_length))\n",
        "model.add(Bidirectional(keras.layers.LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(keras.layers.LSTM(32, return_sequences=True)))\n",
        "model.add(Flatten()),\n",
        "Dropout(0.5),\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics = METRICS)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CvO0oEH0-NfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_padded_NN, y_train, epochs=1, validation_data=(X_test_padded_NN, y_test), verbose = 1)"
      ],
      "metadata": {
        "id": "MnBakSEX-Nhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pred = model.predict(X_train_padded_NN)\n",
        "X_test_pred = model.predict(X_test_padded_NN)"
      ],
      "metadata": {
        "id": "D7Cmkpno-Njx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#first is category 0 then catgory 1. Check:\n",
        "#np.logical_and((y_test ==1),np.array([x[0] for x in (round(X_test_p) == 1).values])).sum()\n",
        "\n",
        "results = confusion_matrix(y_test, round(X_test_p))\n",
        "print(results)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "ax = sns.heatmap(results, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['False','True'])\n",
        "ax.yaxis.set_ticklabels(['False','True'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8cjzMdp-NnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5KSE_7NY-iI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R12ufKkd-iLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vm5iXRS--iNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BZCLpTWN-iPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ra0yHDDJ-iR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t4kqGoUW-iUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kC5X-qMz-iVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bvuRsGEt-iZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "def knn_inputer(X):\n",
        "\n",
        "    X[X.isnull()] = np.nan\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    X_im = imputer.fit_transform(X)\n",
        "    return pd.DataFrame(X_im, columns = list(X.columns))\n",
        "\n",
        "def process_dat(X):\n",
        "  \n",
        "  X_1 = X.loc[:,['title','developer','publisher']]\n",
        "  X_1 = pd.get_dummies(X_1)\n",
        "  X_1 = pd.concat([X_1,X.loc[:,['review_id','year']]], axis = 1)\n",
        "\n",
        "  return X_1\n",
        "\n",
        "X_trainm = process_dat(X_train)\n",
        "X_trainm = knn_inputer(X_trainm)\n",
        "X_trainm['predictions'] = [x[0] for x in X_train_pred.tolist()]\n",
        "\n",
        "X_testm = process_dat(X_test)\n",
        "X_testm = knn_inputer(X_testm)\n",
        "X_testm['predictions'] = [x[0] for x in X_test_pred.tolist()]\n",
        "\n",
        "print(X_trainm.isnull().sum())\n",
        "print(X_testm.isnull().sum())"
      ],
      "metadata": {
        "id": "Z4_xUhPlvO6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "chp8ssrZvO8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iE4dbmzJvO-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}